# CoursesNotes

## [Generative AI: Working with Large Language Models - Learning about Large Language Models](https://github.com/gjkaur/CoursesNotes/tree/main/Generative%20AI%3A%20Working%20with%20Large%20Language%20Models)

Here are the key points from the provided text:

1. **Transformers and Large Language Models (LLMs)**:
   - BERT and GPT-3 are examples of large language models.
   - These models are based on transformer architecture, proposed in the 2017 paper "Attention is All You Need" by Google researchers.
   - Transformers have significantly influenced NLP.

2. **Parameters in LLMs**:
   - Parameters are values in a model updated during training.
   - Large language models have millions to billions of parameters and are trained on vast amounts of data.

3. **Focus Period**:
   - The focus is on models released between May 2020 and July 2022, following GPT-3's release.

4. **Notable LLMs Released**:
   - **Google Research**: GLaM and PaLM.
   - **DeepMind**: Gopher and Chinchilla.
   - **Microsoft and Nvidia**: Megatron-Turing NLG.
   - **Meta AI**: Open Pre-trained Transformer.
   - **Hugging Face**: Coordinated research effort resulting in the BLOOM model, involving over 1000 researchers.

5. **Availability of LLMs**:
   - Meta AI and Hugging Face have worked to make large language models accessible to researchers outside of big tech companies.

6. **Applications**:
   - The text implies that LLMs are widely used in various production applications, though specific examples are not detailed in this excerpt.
